\section{Parallel matrix multiplication}\label{sec:multiplication}

The first task asks for the implementation of a parallel matrix by matrix multiplication %
algorithm in \textit{C} using OpenMP. The goal of the task is to check the scalability %
of the code and evaluate its efficiency.

The most trivial method is called "row by column" and involves computing each element of the %
resulting matrix by taking the dot product of a row of the first matrix and the %
corresponding column of the second one. Other algorithms and %
techniques are more complex\cite{algorithms, ds-algorithms} and exploit the hardware %
of the machine (registers, cache and memory alignment, etc.) or the content of %
the matrices to get better performance. Such methodologies are %
"row by row", "column by column", "Strassen's", and many others.

For the sake of simplicity, I choose to implement the "row by column" method. %
It was designed with dense matrices in mind but works fine with sparse matrices %
without exploiting their benefits, like reduced memory usage and data reutilization. %
I opted for this method as, in my assessment, the more complex ones surpass the %
scope of this assignment.

The algorithm was implemented sequentially and later I introduced %
OpenMP directives to obtain the parallel code.\\%
Source code \ref{code:multiplication} shows the parallelized function with the %
directives used. The choice of which ones to use was made after reading %
the professor's material on the subject \cite{prof-slides}, OpenMP's 3.1 %
documentation \cite{openmp-cs, openmp-docs}, and trying various combinations. %
In the end, I decided to use the following: %
\texttt{\#pragma omp parallel for collapse(2) reduction(+:temp)}. In particular:
\begin{itemize}
    \item \texttt{\#pragma omp parallel}: creates a parallel region in which all %
        code is distributed among the specified number of CPUs. The amount is %
        controlled externally by the \texttt{OMP\_NUM\_THREADS} environment %
        variable.
    \item \texttt{for}: \texttt{for}-loops are going to be distributed between available %
        CPUs.
    \item \texttt{collapse(2)} applies to the previous \texttt{for} directive and %
        specifies the number of nested loops to divide among the CPUs.
    \item \texttt{reduction(+:temp)} specifies the variable and the operation on %
        which to perform the reduction.
\end{itemize}

\input{assets/code/multiplication}

Before the final run on the University's HPC cluster (GCC 4.8.5, OpenMP 3.1), %
the code was tested and debugged on my local machine, a laptop equipped with an %
Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} %
i5-8300H at 2.30GHz and 16GB of RAM running Fedora 39 (GCC 13.2.1, OpenMP 4.5).\\%
As an effort to simplify the compilation and execution of the various iterations %
of the code with different values, I wrote a \textit{bash} script which later %
became the PBS job submission file. This script is responsible for creating the %
necessary directories on the file system, for compilation, and execution with %
the various values. The results are returned in the form of a series of %
\texttt{.CSV} files which are easier to work on with other programs.

\subsection*{Results analysis}
The implemented algorithm was run with dense square matrices of sizes ranging %
from 200 to 2000 with up to 32 CPUs. The node responsible for the computation was %
\texttt{hpc-c11-node22.unitn.it} equipped with an Intel\textsuperscript{\textregistered} %
Xeon\textsuperscript{\textregistered} Gold 6252N CPU running at 2.30GHz and 256GB %
of RAM. The obtained execution times are reported in Table \ref{table:multiplication} %
alongside the FLOPS calculated using Equation \ref{eq:flops} and expressed as %
GFLOPS.

\begin{equation}
    \label{eq:flops}
    \textnormal{FLOPS}=\frac{%
            2 \cdot \textnormal{A rows} \cdot \textnormal{A columns} \cdot %
            \textnormal{B columns}
        }{\textnormal{execution time}}=
        \frac{%
            2 \cdot \textnormal{size}^3}{\textnormal{execution time}}
\end{equation}

\input{assets/tables/multiplication}

The parallel scalability of the algorithm was evaluated using the FLOPS as the %
performance metric. Plot \ref{plot:mult-performance}, Plot \ref{plot:mult-speedup}, %
and Plot \ref{plot:mult-efficiency} allow us to see a visual representation of the %
performance, the speedup, and the efficiency, respectively.

\input{assets/plots/mult-performance}
\input{assets/plots/mult-speedup}
\input{assets/plots/mult-efficiency}

It is easy to see that the execution has a better performance (Plot \ref{plot:mult-performance}) %
the more CPUs it can utilize because of the higher number of operations it can %
execute per second. The same consideration is also valid for the speedup %
(Plot \ref{plot:mult-speedup}): the more CPUs, the faster the calculations are %
executed. \\
The spikes observable in the efficiency graph (plot \ref{plot:mult-efficiency}) %
are because parallel systems are nonlinearly scalable. We must choose %
either a constant run time or a constant efficiency when scaling up the problem %
\cite{scalability}. The general downward slope can be interrupted by undefined %
peaks caused by imbalance problems. In case in exam, it is particularly evident with %
$200\times200$ matrices (red line) and $400\times400$ matrices (blue line).
